{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b14255",
   "metadata": {
    "papermill": {
     "duration": 0.031784,
     "end_time": "2022-03-07T14:36:18.145278",
     "exception": false,
     "start_time": "2022-03-07T14:36:18.113494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "- feedback-prize-2021 コンペ。\n",
    "- 公開ノートブックの特徴量とロジスティック回帰を書き換え。\n",
    "\n",
    "TODO\n",
    "- CountVectorizer から TF-IDF に書き換え。\n",
    "- モデルを LightGBM に書き換え。\n",
    "- \"No Class\" の補完。\n",
    "- 文をつなぐ方針のアルゴリズムから、文を分裂させることもあるアルゴリズムにできる？\n",
    "\n",
    "Ref.\n",
    "- [Expanding Sentences Window - Logistic Regression | Kaggle (@samir95)](https://www.kaggle.com/samir95/expanding-sentences-window-logistic-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b9d16f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:18.218831Z",
     "iopub.status.busy": "2022-03-07T14:36:18.218256Z",
     "iopub.status.idle": "2022-03-07T14:36:18.220990Z",
     "shell.execute_reply": "2022-03-07T14:36:18.220459Z",
     "shell.execute_reply.started": "2022-03-07T10:34:32.032371Z"
    },
    "papermill": {
     "duration": 0.044914,
     "end_time": "2022-03-07T14:36:18.221125",
     "exception": false,
     "start_time": "2022-03-07T14:36:18.176211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "SEED = 4121995\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1461cb8",
   "metadata": {
    "papermill": {
     "duration": 0.050837,
     "end_time": "2022-03-07T14:36:18.314184",
     "exception": false,
     "start_time": "2022-03-07T14:36:18.263347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The approach I'll use in this notebook is to build the bare minimum training and validation set and an algorithm maximizing the likelihood of an n-gram of sentences (or words) to be one of a discourse types using a RandomForestClassifier or a NaiveBayesClassifier on TFIDF or CountVectorized datasets.\n",
    "\n",
    "#### I'll do this without exploring the dataset, as I'm more interested in making a validation pipeline and an algorithm that I have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0223a26",
   "metadata": {
    "papermill": {
     "duration": 0.030291,
     "end_time": "2022-03-07T14:36:18.379731",
     "exception": false,
     "start_time": "2022-03-07T14:36:18.349440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Creating training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6dded4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:18.445436Z",
     "iopub.status.busy": "2022-03-07T14:36:18.444655Z",
     "iopub.status.idle": "2022-03-07T14:36:19.998099Z",
     "shell.execute_reply": "2022-03-07T14:36:19.999321Z",
     "shell.execute_reply.started": "2022-03-07T10:34:32.065598Z"
    },
    "papermill": {
     "duration": 1.58891,
     "end_time": "2022-03-07T14:36:19.999531",
     "exception": false,
     "start_time": "2022-03-07T14:36:18.410621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505476b",
   "metadata": {
    "papermill": {
     "duration": 0.05245,
     "end_time": "2022-03-07T14:36:20.115102",
     "exception": false,
     "start_time": "2022-03-07T14:36:20.062652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I'll make a validation set with a number of essays that is equal to the number of essays in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f242d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:20.229924Z",
     "iopub.status.busy": "2022-03-07T14:36:20.228490Z",
     "iopub.status.idle": "2022-03-07T14:36:20.931194Z",
     "shell.execute_reply": "2022-03-07T14:36:20.930706Z",
     "shell.execute_reply.started": "2022-03-07T10:34:33.602266Z"
    },
    "papermill": {
     "duration": 0.765195,
     "end_time": "2022-03-07T14:36:20.931329",
     "exception": false,
     "start_time": "2022-03-07T14:36:20.166134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/kaggle/input/feedback-prize-2021/test' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa13a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:20.999405Z",
     "iopub.status.busy": "2022-03-07T14:36:20.998662Z",
     "iopub.status.idle": "2022-03-07T14:36:21.951449Z",
     "shell.execute_reply": "2022-03-07T14:36:21.950767Z",
     "shell.execute_reply.started": "2022-03-07T10:34:34.385808Z"
    },
    "papermill": {
     "duration": 0.9885,
     "end_time": "2022-03-07T14:36:21.951585",
     "exception": false,
     "start_time": "2022-03-07T14:36:20.963085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15594\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/kaggle/input/feedback-prize-2021/train' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32643c",
   "metadata": {
    "papermill": {
     "duration": 0.031797,
     "end_time": "2022-03-07T14:36:22.015200",
     "exception": false,
     "start_time": "2022-03-07T14:36:21.983403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training data has 15594 essays, which means that we can possibly spare more than 5 essays. But I don't know if we should.\n",
    "\n",
    "The regular approach would be to split the dataset into an 80/20 training/validation split. <s>but until now this notebook hasn't made any rigorous algorithm, so I'll stick with 5 essays just to make it run.</s> \n",
    "\n",
    "<s>After I make sure that the pipeline is working, I can look into making a robust validation set that reflects public leaderboard score, and indicates how well we can do in the private dataset.</s>\n",
    "\n",
    "Evaluation one essay takes around 113 ms, so in order to minimize waiting and still be able to evaluate results, I'll use around 50 essays so that evaluation doesn't exceed 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63482c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.085433Z",
     "iopub.status.busy": "2022-03-07T14:36:22.082724Z",
     "iopub.status.idle": "2022-03-07T14:36:22.088294Z",
     "shell.execute_reply": "2022-03-07T14:36:22.087878Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.318628Z"
    },
    "papermill": {
     "duration": 0.04145,
     "end_time": "2022-03-07T14:36:22.088404",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.046954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# id をランダムに n 個選んで valid にする\n",
    "def split_essays(train_df, n):\n",
    "    if isinstance(n, float):\n",
    "        n = int(len(train_df.id.unique()) * n)\n",
    "    val_ids = np.random.choice(train_df.id.unique(), n, False)\n",
    "    train_df, val_df = train_df[~train_df.id.isin(val_ids)], train_df[train_df.id.isin(val_ids)]\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90316aef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.158048Z",
     "iopub.status.busy": "2022-03-07T14:36:22.157278Z",
     "iopub.status.idle": "2022-03-07T14:36:22.224827Z",
     "shell.execute_reply": "2022-03-07T14:36:22.225271Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.326579Z"
    },
    "papermill": {
     "duration": 0.105466,
     "end_time": "2022-03-07T14:36:22.225415",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.119949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143379, 8), (914, 8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = split_essays(train_df, 100)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f5c30",
   "metadata": {
    "papermill": {
     "duration": 0.031639,
     "end_time": "2022-03-07T14:36:22.289068",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.257429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now of course I don't want to use the whole training dataset right now, so I'll make another function to sample a small part for development purposes. I can use the previous function that I made, but I'll discard the training set it creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a9625f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.357248Z",
     "iopub.status.busy": "2022-03-07T14:36:22.356597Z",
     "iopub.status.idle": "2022-03-07T14:36:22.359157Z",
     "shell.execute_reply": "2022-03-07T14:36:22.359556Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.405351Z"
    },
    "papermill": {
     "duration": 0.038913,
     "end_time": "2022-03-07T14:36:22.359691",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.320778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143379, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _, dev_df = split_essays(train_df, 10000)\n",
    "dev_df = train_df\n",
    "dev_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e072e",
   "metadata": {
    "papermill": {
     "duration": 0.032221,
     "end_time": "2022-03-07T14:36:22.423792",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.391571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "One thing that we need to add to the training set is parapgraphs or blocks of text which have no classification, and then we should add a label to them in order to train the classifier to also predict gaps in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c6c36f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.502943Z",
     "iopub.status.busy": "2022-03-07T14:36:22.502086Z",
     "iopub.status.idle": "2022-03-07T14:36:22.504192Z",
     "shell.execute_reply": "2022-03-07T14:36:22.504558Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.412003Z"
    },
    "papermill": {
     "duration": 0.048958,
     "end_time": "2022-03-07T14:36:22.504696",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.455738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ラベルがついていないテキストを補完する\n",
    "\n",
    "# These functions are inspired by this amazing notebook \n",
    "# https://www.kaggle.com/erikbruin/nlp-on-student-writing-eda\n",
    "\n",
    "def get_unique_ids(df):\n",
    "    return df.id.unique()\n",
    "\n",
    "def filter_essay(df, essay_id):\n",
    "    return df.query('id == @essay_id').reset_index(drop=True)\n",
    "\n",
    "def read_essay_txt(essay_id, path='train'):\n",
    "    essay_file_path = f\"../input/feedback-prize-2021/{path}/{essay_id}.txt\"\n",
    "    with open(essay_file_path, 'r') as essay_file:\n",
    "        return essay_file.read()\n",
    "        \n",
    "def add_gap_rows_essay(df, essay_id, path):\n",
    "    \n",
    "    essay_df = filter_essay(df, essay_id)\n",
    "    essay_txt = read_essay_txt(essay_id, path)\n",
    "    \n",
    "    for index, row in essay_df.iterrows():\n",
    "        if index == essay_df.index[0]: \n",
    "            continue\n",
    "            \n",
    "        current_discourse_start = int(row['discourse_start'])\n",
    "        current_discourse_end = int(row['discourse_end'])\n",
    "        previous_discourse_start = int(essay_df.loc[index - 1, 'discourse_start'])\n",
    "        previous_discourse_end = int(essay_df.loc[index - 1, 'discourse_end'])\n",
    "\n",
    "        if previous_discourse_end != current_discourse_start - 1 and previous_discourse_end != current_discourse_start:\n",
    "            current_predstring = row['predictionstring']\n",
    "            previous_predstring = essay_df.loc[index - 1, 'predictionstring']\n",
    "\n",
    "            current_predstring_first_token = int(current_predstring.split()[0])\n",
    "            previous_predstring_last_token = int(previous_predstring.split()[-1])\n",
    "            \n",
    "            gap_tokens_list = np.arange(previous_predstring_last_token + 1,\n",
    "                                        current_predstring_first_token).tolist()\n",
    "\n",
    "            gap_row = {}  \n",
    "            gap_row['id'] = row['id']\n",
    "            gap_row['discourse_id'] = row['discourse_id']\n",
    "            gap_row['discourse_start'] = previous_discourse_end + 1\n",
    "            gap_row['discourse_end'] = current_discourse_start - 1\n",
    "            gap_row['discourse_text'] = essay_txt[previous_discourse_end+1: current_discourse_start]\n",
    "            gap_row['discourse_type'] = 'Gap'\n",
    "            gap_row['discourse_type_num'] = 'Gap'\n",
    "            gap_row['predictionstring'] = ' '.join([str(token) for token in gap_tokens_list])\n",
    "            \n",
    "            essay_df = essay_df.append(pd.Series(gap_row), ignore_index=True)\n",
    "    \n",
    "    essay_df = essay_df.sort_values('discourse_start').reset_index(drop=True)\n",
    "    return essay_df\n",
    "\n",
    "def add_gap_rows_df(df, path):\n",
    "    new_df = None\n",
    "    essay_ids = get_unique_ids(df)\n",
    "    \n",
    "    for essay_id in essay_ids:\n",
    "        essay_df = add_gap_rows_essay(df, essay_id, path)\n",
    "        new_df = pd.concat([new_df, essay_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    return new_df           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aade0957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.576835Z",
     "iopub.status.busy": "2022-03-07T14:36:22.575729Z",
     "iopub.status.idle": "2022-03-07T14:36:22.625790Z",
     "shell.execute_reply": "2022-03-07T14:36:22.625287Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.431480Z"
    },
    "papermill": {
     "duration": 0.087913,
     "end_time": "2022-03-07T14:36:22.625939",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.538026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 950 µs, total: 16.6 ms\n",
      "Wall time: 30 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>People are debating whether if drivers should ...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>455.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>I also think that you shouldn't use your phone...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>77 78 79 80 81 82 83 84 85 86 87 88 89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>528.0</td>\n",
       "      <td>568.0</td>\n",
       "      <td>because it can cause vehicle collisions</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>90 91 92 93 94 95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>569.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>slow reaction time,</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 2</td>\n",
       "      <td>96 97 98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>589.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>and fatal injuries.</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 3</td>\n",
       "      <td>99 100 101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>610.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>herefore, driving can cause many accidents tha...</td>\n",
       "      <td>Gap</td>\n",
       "      <td>Gap</td>\n",
       "      <td>102 103 104 105 106 107 108 109 110 111 112 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>937.0</td>\n",
       "      <td>\\nThe first reason why the use of cell phones ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 4</td>\n",
       "      <td>133 134 135 136 137 138 139 140 141 142 143 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>937.0</td>\n",
       "      <td>1403.0</td>\n",
       "      <td>Most vehicle collisions happen when the drive...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>158 159 160 161 162 163 164 165 166 167 168 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>The second reason why you shouldn't operate a ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 5</td>\n",
       "      <td>234 235 236 237 238 239 240 241 242 243 244 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>Reaction time is the measure of how quickly an...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>251 252 253 254 255 256 257 258 259 260 261 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>2043.0</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>The last reason you shouldn't drive while usin...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 6</td>\n",
       "      <td>345 346 347 348 349 350 351 352 353 354 355 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622838e+12</td>\n",
       "      <td>2188.0</td>\n",
       "      <td>2877.0</td>\n",
       "      <td>The affects can be physical, emotional, or psy...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 3</td>\n",
       "      <td>368 369 370 371 372 373 374 375 376 377 378 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>1.622839e+12</td>\n",
       "      <td>2878.0</td>\n",
       "      <td>3513.0</td>\n",
       "      <td>In conclusion, people shouldn't use cellphones...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>478 479 480 481 482 483 484 485 486 487 488 48...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  discourse_id  discourse_start  discourse_end  \\\n",
       "0   E05C7F5C1156  1.622838e+12              0.0          455.0   \n",
       "1   E05C7F5C1156  1.622838e+12            455.0          527.0   \n",
       "2   E05C7F5C1156  1.622838e+12            528.0          568.0   \n",
       "3   E05C7F5C1156  1.622838e+12            569.0          588.0   \n",
       "4   E05C7F5C1156  1.622838e+12            589.0          609.0   \n",
       "5   E05C7F5C1156  1.622838e+12            610.0          781.0   \n",
       "6   E05C7F5C1156  1.622838e+12            782.0          937.0   \n",
       "7   E05C7F5C1156  1.622838e+12            937.0         1403.0   \n",
       "8   E05C7F5C1156  1.622838e+12           1404.0         1506.0   \n",
       "9   E05C7F5C1156  1.622838e+12           1507.0         2042.0   \n",
       "10  E05C7F5C1156  1.622838e+12           2043.0         2188.0   \n",
       "11  E05C7F5C1156  1.622838e+12           2188.0         2877.0   \n",
       "12  E05C7F5C1156  1.622839e+12           2878.0         3513.0   \n",
       "\n",
       "                                       discourse_text        discourse_type  \\\n",
       "0   People are debating whether if drivers should ...                  Lead   \n",
       "1   I also think that you shouldn't use your phone...              Position   \n",
       "2            because it can cause vehicle collisions                  Claim   \n",
       "3                                 slow reaction time,                 Claim   \n",
       "4                                and fatal injuries.                  Claim   \n",
       "5   herefore, driving can cause many accidents tha...                   Gap   \n",
       "6   \\nThe first reason why the use of cell phones ...                 Claim   \n",
       "7    Most vehicle collisions happen when the drive...              Evidence   \n",
       "8   The second reason why you shouldn't operate a ...                 Claim   \n",
       "9   Reaction time is the measure of how quickly an...              Evidence   \n",
       "10  The last reason you shouldn't drive while usin...                 Claim   \n",
       "11  The affects can be physical, emotional, or psy...              Evidence   \n",
       "12  In conclusion, people shouldn't use cellphones...  Concluding Statement   \n",
       "\n",
       "        discourse_type_num                                   predictionstring  \n",
       "0                   Lead 1  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "1               Position 1             77 78 79 80 81 82 83 84 85 86 87 88 89  \n",
       "2                  Claim 1                                  90 91 92 93 94 95  \n",
       "3                  Claim 2                                           96 97 98  \n",
       "4                  Claim 3                                         99 100 101  \n",
       "5                      Gap  102 103 104 105 106 107 108 109 110 111 112 11...  \n",
       "6                  Claim 4  133 134 135 136 137 138 139 140 141 142 143 14...  \n",
       "7               Evidence 1  158 159 160 161 162 163 164 165 166 167 168 16...  \n",
       "8                  Claim 5  234 235 236 237 238 239 240 241 242 243 244 24...  \n",
       "9               Evidence 2  251 252 253 254 255 256 257 258 259 260 261 26...  \n",
       "10                 Claim 6  345 346 347 348 349 350 351 352 353 354 355 35...  \n",
       "11              Evidence 3  368 369 370 371 372 373 374 375 376 377 378 37...  \n",
       "12  Concluding Statement 1  478 479 480 481 482 483 484 485 486 487 488 48...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Testing on one discourse\n",
    "add_gap_rows_essay(dev_df, dev_df.id.values[30], 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b6a248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:36:22.707874Z",
     "iopub.status.busy": "2022-03-07T14:36:22.707156Z",
     "iopub.status.idle": "2022-03-07T14:42:28.366051Z",
     "shell.execute_reply": "2022-03-07T14:42:28.366521Z",
     "shell.execute_reply.started": "2022-03-07T10:34:35.474598Z"
    },
    "papermill": {
     "duration": 365.704537,
     "end_time": "2022-03-07T14:42:28.366669",
     "exception": false,
     "start_time": "2022-03-07T14:36:22.662132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 48s, sys: 25.5 s, total: 5min 14s\n",
      "Wall time: 6min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 8min 29s\n",
    "dev_df = add_gap_rows_df(dev_df, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c48d08f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:28.440874Z",
     "iopub.status.busy": "2022-03-07T14:42:28.440263Z",
     "iopub.status.idle": "2022-03-07T14:42:29.539938Z",
     "shell.execute_reply": "2022-03-07T14:42:29.540586Z",
     "shell.execute_reply.started": "2022-03-07T10:40:45.209508Z"
    },
    "papermill": {
     "duration": 1.140548,
     "end_time": "2022-03-07T14:42:29.540738",
     "exception": false,
     "start_time": "2022-03-07T14:42:28.400190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 772 ms, sys: 13.1 ms, total: 785 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# 1sec\n",
    "val_df = add_gap_rows_df(val_df, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf41dc",
   "metadata": {
    "papermill": {
     "duration": 0.033864,
     "end_time": "2022-03-07T14:42:29.608382",
     "exception": false,
     "start_time": "2022-03-07T14:42:29.574518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It seems that gaps aren't always sentences, so I think that dropping gap lines that aren't full sentences or paragraphs could be beneficial as the model will always be passed TFIDF or Vectorized sentences, unless the approach is used but with maximizing the likelihood of ngrams of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf7cf5b",
   "metadata": {
    "papermill": {
     "duration": 0.033641,
     "end_time": "2022-03-07T14:42:29.675916",
     "exception": false,
     "start_time": "2022-03-07T14:42:29.642275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Algorithm\n",
    "\n",
    "What I have in a mind is a simple algorithm that maximizes the likelihood of a TFIDF or Bag of words of sequences of sentences (or words) of being one of the presented discourse types.\n",
    "\n",
    "I don't know much about NLP, but this is the easiest way I can think about. \n",
    "\n",
    "For example, let's say that we an essay, the algorithm would split this essay into sentences, then it would classify the first sentence alone and take it's maximum prediction as a baseline. \n",
    "\n",
    "The algorithm would then add another sentence classify the two sentences together, and then add another and another. Hypothetically, since I will be training the simple model on block of discourse types, the likelihood would increase with increasing sentences, until it starts decreasing, and then we can select the block which maximized the likelihood of correct prediction according to the model.\n",
    "\n",
    "Then the algorithm would iterate again from the sentence following the previously predicted block of text.\n",
    "\n",
    "The core of this approach can be used with any complex model, but I don't know if it would be suitable or not, since probably deep learning models could be capable of more without this brute forcing attempt, but I could be wrong since I don't know much about NLP.\n",
    "\n",
    "And of course this approach is naive, since it won't learn about the context of the essay, and hence won't be able to predict two evidence sentences following each other for example, and might consider them as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf3cb52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:29.748050Z",
     "iopub.status.busy": "2022-03-07T14:42:29.747180Z",
     "iopub.status.idle": "2022-03-07T14:42:29.749419Z",
     "shell.execute_reply": "2022-03-07T14:42:29.748927Z",
     "shell.execute_reply.started": "2022-03-07T10:40:46.340906Z"
    },
    "papermill": {
     "duration": 0.039458,
     "end_time": "2022-03-07T14:42:29.749527",
     "exception": false,
     "start_time": "2022-03-07T14:42:29.710069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 例えば、エッセイがあったとすると、このアルゴリズムはこのエッセイを文に分割し、\n",
    "# 最初の文だけを分類し、その最大予測値を基準値とします。\n",
    "# \n",
    "# 次に別の文を追加し、2つの文を一緒に分類し、さらに別の文を追加する。\n",
    "# このような場合、単純なモデルを談話タイプのブロックに対して学習させるので、\n",
    "# 尤度は文の増加とともに増加し、減少し始め、そしてモデルに従って正しい予測の尤度を\n",
    "# 最大にするブロックを選択することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684547d",
   "metadata": {
    "papermill": {
     "duration": 0.033286,
     "end_time": "2022-03-07T14:42:29.816476",
     "exception": false,
     "start_time": "2022-03-07T14:42:29.783190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing the dataset\n",
    "\n",
    "TFIDF or Bag of Words could be used, <s>but I'll use a simple Bag of Words right now.</s> and I'll use TFIDF.\n",
    "\n",
    "The data provides two features regarding position of the text which is discourse_start and discourse_end. I want to use them, but I'm think it would be easier to another feature which is the start token, since my model uses Bag of Words, so it would be easier to implement it into the prediction pipeline of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d76a9790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:29.910513Z",
     "iopub.status.busy": "2022-03-07T14:42:29.909681Z",
     "iopub.status.idle": "2022-03-07T14:42:31.230404Z",
     "shell.execute_reply": "2022-03-07T14:42:31.229974Z",
     "shell.execute_reply.started": "2022-03-07T10:40:46.346312Z"
    },
    "papermill": {
     "duration": 1.380563,
     "end_time": "2022-03-07T14:42:31.230558",
     "exception": false,
     "start_time": "2022-03-07T14:42:29.849995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>start_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \\\n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...   \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...   \n",
       "\n",
       "   start_token  \n",
       "0            1  \n",
       "1           45  \n",
       "2           60  \n",
       "3           76  \n",
       "4          139  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add starting token feature\n",
    "dev_df['start_token'] = dev_df['predictionstring'].str.split().str[-1].shift(1).fillna(0).astype(int) + 1\n",
    "val_df['start_token'] = val_df['predictionstring'].str.split().str[-1].shift(1).fillna(0).astype(int) + 1\n",
    "\n",
    "dev_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd133dcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:31.304732Z",
     "iopub.status.busy": "2022-03-07T14:42:31.303971Z",
     "iopub.status.idle": "2022-03-07T14:42:39.740348Z",
     "shell.execute_reply": "2022-03-07T14:42:39.741090Z",
     "shell.execute_reply.started": "2022-03-07T10:40:47.705688Z"
    },
    "papermill": {
     "duration": 8.475702,
     "end_time": "2022-03-07T14:42:39.741252",
     "exception": false,
     "start_time": "2022-03-07T14:42:31.265550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 s, sys: 571 ms, total: 1.8 s\n",
      "Wall time: 8.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Add full text length feature\n",
    "train_essays = os.listdir('/kaggle/input/feedback-prize-2021/train/')\n",
    "test_essays = os.listdir('/kaggle/input/feedback-prize-2021/test/')\n",
    "\n",
    "train_essays_length = {id_.rstrip('.txt'): len(read_essay_txt(id_.rstrip('.txt')).split()) for id_ in train_essays}\n",
    "test_essays_length = {id_.rstrip('.txt'): len(read_essay_txt(id_.rstrip('.txt'), 'test').split()) for id_ in test_essays}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ea64e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:39.822729Z",
     "iopub.status.busy": "2022-03-07T14:42:39.813175Z",
     "iopub.status.idle": "2022-03-07T14:42:39.868014Z",
     "shell.execute_reply": "2022-03-07T14:42:39.868429Z",
     "shell.execute_reply.started": "2022-03-07T10:40:54.865258Z"
    },
    "papermill": {
     "duration": 0.092018,
     "end_time": "2022-03-07T14:42:39.868571",
     "exception": false,
     "start_time": "2022-03-07T14:42:39.776553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>start_token</th>\n",
       "      <th>len_essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "      <td>45</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>60</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "      <td>76</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "      <td>139</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>887.0</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>That's why there's a thing that's called no te...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 3</td>\n",
       "      <td>163 164 165 166 167 168 169 170 171 172 173 17...</td>\n",
       "      <td>163</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>1151.0</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>Sometimes on the news there is either an accid...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 4</td>\n",
       "      <td>211 212 213 214 215 216 217 218 219 220 221 22...</td>\n",
       "      <td>211</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>Phones are fine to use and it's also the best ...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 2</td>\n",
       "      <td>282 283 284 285 286 287 288 289 290 291 292 29...</td>\n",
       "      <td>282</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>If you go through a problem and you can't find...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 5</td>\n",
       "      <td>297 298 299 300 301 302 303 304 305 306 307 30...</td>\n",
       "      <td>297</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>1891.0</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>The news always updated when people do somethi...</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>Concluding Statement 1</td>\n",
       "      <td>355 356 357 358 359 360 361 362 363 364 365 36...</td>\n",
       "      <td>355</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "5  423A1CA112E2  1.622628e+12            887.0         1150.0   \n",
       "6  423A1CA112E2  1.622628e+12           1151.0         1533.0   \n",
       "7  423A1CA112E2  1.622628e+12           1534.0         1602.0   \n",
       "8  423A1CA112E2  1.622628e+12           1603.0         1890.0   \n",
       "9  423A1CA112E2  1.622628e+12           1891.0         2027.0   \n",
       "\n",
       "                                      discourse_text        discourse_type  \\\n",
       "0  Modern humans today are always on their phone....                  Lead   \n",
       "1  They are some really bad consequences when stu...              Position   \n",
       "2  Some certain areas in the United States ban ph...              Evidence   \n",
       "3  When people have phones, they know about certa...              Evidence   \n",
       "4  Driving is one of the way how to get around. P...                 Claim   \n",
       "5  That's why there's a thing that's called no te...              Evidence   \n",
       "6  Sometimes on the news there is either an accid...              Evidence   \n",
       "7  Phones are fine to use and it's also the best ...                 Claim   \n",
       "8  If you go through a problem and you can't find...              Evidence   \n",
       "9  The news always updated when people do somethi...  Concluding Statement   \n",
       "\n",
       "       discourse_type_num                                   predictionstring  \\\n",
       "0                  Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "1              Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
       "2              Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "3              Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...   \n",
       "4                 Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...   \n",
       "5              Evidence 3  163 164 165 166 167 168 169 170 171 172 173 17...   \n",
       "6              Evidence 4  211 212 213 214 215 216 217 218 219 220 221 22...   \n",
       "7                 Claim 2  282 283 284 285 286 287 288 289 290 291 292 29...   \n",
       "8              Evidence 5  297 298 299 300 301 302 303 304 305 306 307 30...   \n",
       "9  Concluding Statement 1  355 356 357 358 359 360 361 362 363 364 365 36...   \n",
       "\n",
       "   start_token  len_essay  \n",
       "0            1        379  \n",
       "1           45        379  \n",
       "2           60        379  \n",
       "3           76        379  \n",
       "4          139        379  \n",
       "5          163        379  \n",
       "6          211        379  \n",
       "7          282        379  \n",
       "8          297        379  \n",
       "9          355        379  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df['len_essay'] = dev_df['id'].map(train_essays_length)\n",
    "val_df['len_essay'] = val_df['id'].map(train_essays_length)\n",
    "dev_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965ea595",
   "metadata": {
    "papermill": {
     "duration": 0.034834,
     "end_time": "2022-03-07T14:42:39.939652",
     "exception": false,
     "start_time": "2022-03-07T14:42:39.904818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I'll make a transformer which expects starting token feature and text, then it uses them to calculate the ending token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1db4f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:40.022528Z",
     "iopub.status.busy": "2022-03-07T14:42:40.021813Z",
     "iopub.status.idle": "2022-03-07T14:42:40.901516Z",
     "shell.execute_reply": "2022-03-07T14:42:40.901058Z",
     "shell.execute_reply.started": "2022-03-07T10:40:54.940673Z"
    },
    "papermill": {
     "duration": 0.927029,
     "end_time": "2022-03-07T14:42:40.901650",
     "exception": false,
     "start_time": "2022-03-07T14:42:39.974621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class ExtraFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X['end_token'] = X['start_token'] + X['discourse_text'].str.split().str.len()\n",
    "        X['tokens_from_start_to_end'] = X['end_token'] - X['start_token'] + 1\n",
    "        X['tokens_from_start_to_finish'] = X['len_essay'] - X['start_token'] + 1\n",
    "        X['tokens_from_end_to_finish'] = X['len_essay'] - X['end_token'] + 1\n",
    "        X['percent_read_before'] = X['start_token'] / X['len_essay']\n",
    "        X['percent_read_now'] = X['end_token'] / X['len_essay']\n",
    "        X['percent_remaining'] = (X['len_essay'] - X['end_token']) / X['len_essay']\n",
    "        X['percent_sentence'] = (X['end_token'] - X['start_token'] + 1) / X['len_essay']\n",
    "        \n",
    "        feats = ['percent_read_before', 'percent_read_now', 'percent_remaining', 'percent_sentence']\n",
    "        return X[feats].values\n",
    "    \n",
    "\n",
    "def preprocess_data(X, pipeline=None):\n",
    "    if not pipeline:\n",
    "        \n",
    "        vectorizer_pipeline = Pipeline([\n",
    "            ('vectorizer', CountVectorizer(ngram_range=(1, 3), max_features=100000)),\n",
    "#             ('tfidf', TfidfTransformer()),\n",
    "#             ('pca', TruncatedSVD(n_components=0.9))\n",
    "        ])\n",
    "                \n",
    "        pipeline = FeatureUnion([\n",
    "            ('extra_features', ColumnTransformer([('end_token', ExtraFeatures(), ['start_token', 'len_essay', 'discourse_text'])])),\n",
    "            ('vectorizer', ColumnTransformer([('vectorizer', vectorizer_pipeline, 'discourse_text')]))\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X)\n",
    "        \n",
    "    X = pipeline.transform(X)\n",
    "    return X, pipeline\n",
    "\n",
    "\n",
    "def encode_labels(y, encoder=None):\n",
    "    if not encoder:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(y)\n",
    "        \n",
    "    y = encoder.transform(y)\n",
    "    return y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bda198c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:40.979619Z",
     "iopub.status.busy": "2022-03-07T14:42:40.978816Z",
     "iopub.status.idle": "2022-03-07T14:42:40.986105Z",
     "shell.execute_reply": "2022-03-07T14:42:40.986549Z",
     "shell.execute_reply.started": "2022-03-07T10:40:55.773488Z"
    },
    "papermill": {
     "duration": 0.0499,
     "end_time": "2022-03-07T14:42:40.986675",
     "exception": false,
     "start_time": "2022-03-07T14:42:40.936775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>start_token</th>\n",
       "      <th>len_essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \\\n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "\n",
       "   start_token  len_essay  \n",
       "0            1        379  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b16d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:42:41.060583Z",
     "iopub.status.busy": "2022-03-07T14:42:41.059645Z",
     "iopub.status.idle": "2022-03-07T14:43:43.016183Z",
     "shell.execute_reply": "2022-03-07T14:43:43.015501Z",
     "shell.execute_reply.started": "2022-03-07T10:40:55.791211Z"
    },
    "papermill": {
     "duration": 61.994049,
     "end_time": "2022-03-07T14:43:43.016349",
     "exception": false,
     "start_time": "2022-03-07T14:42:41.022300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dev_df[['start_token', 'len_essay', 'discourse_text']], dev_df['discourse_type']\n",
    "X_val, y_val = val_df[['start_token', 'len_essay', 'discourse_text']], val_df['discourse_type']\n",
    "\n",
    "X_train, pipeline = preprocess_data(X_train)\n",
    "X_val, _ = preprocess_data(X_val, pipeline)\n",
    "\n",
    "y_train, encoder = encode_labels(y_train)\n",
    "y_val, _ = encode_labels(y_val, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7b0f3b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:43:43.097007Z",
     "iopub.status.busy": "2022-03-07T14:43:43.096161Z",
     "iopub.status.idle": "2022-03-07T14:43:43.103979Z",
     "shell.execute_reply": "2022-03-07T14:43:43.104395Z",
     "shell.execute_reply.started": "2022-03-07T10:41:57.706552Z"
    },
    "papermill": {
     "duration": 0.05176,
     "end_time": "2022-03-07T14:43:43.104526",
     "exception": false,
     "start_time": "2022-03-07T14:43:43.052766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>start_token</th>\n",
       "      <th>len_essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "      <td>45</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \\\n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
       "\n",
       "   start_token  len_essay  \n",
       "0            1        379  \n",
       "1           45        379  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d418ce6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:43:43.183935Z",
     "iopub.status.busy": "2022-03-07T14:43:43.183320Z",
     "iopub.status.idle": "2022-03-07T14:43:43.269155Z",
     "shell.execute_reply": "2022-03-07T14:43:43.269572Z",
     "shell.execute_reply.started": "2022-03-07T10:41:57.722999Z"
    },
    "papermill": {
     "duration": 0.128095,
     "end_time": "2022-03-07T14:43:43.269708",
     "exception": false,
     "start_time": "2022-03-07T14:43:43.141613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168222, 100004) (1054, 100004)\n",
      "FeatureUnion(transformer_list=[('extra_features',\n",
      "                                ColumnTransformer(transformers=[('end_token',\n",
      "                                                                 ExtraFeatures(),\n",
      "                                                                 ['start_token',\n",
      "                                                                  'len_essay',\n",
      "                                                                  'discourse_text'])])),\n",
      "                               ('vectorizer',\n",
      "                                ColumnTransformer(transformers=[('vectorizer',\n",
      "                                                                 Pipeline(steps=[('vectorizer',\n",
      "                                                                                  CountVectorizer(max_features=100000,\n",
      "                                                                                                  ngram_range=(1,\n",
      "                                                                                                               3)))]),\n",
      "                                                                 'discourse_text')]))])\n",
      "<class 'sklearn.pipeline.FeatureUnion'>\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_char_ngrams', '_char_wb_ngrams', '_check_n_features', '_check_stop_words_consistency', '_check_vocabulary', '_count_vocab', '_get_param_names', '_get_tags', '_limit_features', '_more_tags', '_repr_html_', '_repr_html_inner', '_repr_mimebundle_', '_sort_features', '_validate_data', '_validate_params', '_validate_vocabulary', '_warn_for_unused_params', '_white_spaces', '_word_ngrams', 'analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'get_feature_names', 'get_params', 'get_stop_words', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'set_params', 'stop_words', 'strip_accents', 'token_pattern', 'tokenizer', 'transform', 'vocabulary']\n",
      "{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': 100000, 'min_df': 1, 'ngram_range': (1, 3), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)\n",
    "print(pipeline)\n",
    "print(type(pipeline))\n",
    "# CountVectorizer を確認\n",
    "print(dir(pipeline.transformer_list[1][1].transformers[0][1].steps[0][1]))\n",
    "print(pipeline.transformer_list[1][1].transformers[0][1].steps[0][1].get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6ede9",
   "metadata": {
    "papermill": {
     "duration": 0.035764,
     "end_time": "2022-03-07T14:43:43.341894",
     "exception": false,
     "start_time": "2022-03-07T14:43:43.306130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the core model\n",
    "\n",
    "Since I like Random Forests, I'll stick with a RF Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b596b327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:43:43.418308Z",
     "iopub.status.busy": "2022-03-07T14:43:43.417647Z",
     "iopub.status.idle": "2022-03-07T14:43:45.265223Z",
     "shell.execute_reply": "2022-03-07T14:43:45.264193Z",
     "shell.execute_reply.started": "2022-03-07T10:41:57.819243Z"
    },
    "papermill": {
     "duration": 1.88742,
     "end_time": "2022-03-07T14:43:45.265362",
     "exception": false,
     "start_time": "2022-03-07T14:43:43.377942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5317d1e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:43:45.346178Z",
     "iopub.status.busy": "2022-03-07T14:43:45.345317Z",
     "iopub.status.idle": "2022-03-07T14:43:45.348441Z",
     "shell.execute_reply": "2022-03-07T14:43:45.348857Z",
     "shell.execute_reply.started": "2022-03-07T10:41:59.661197Z"
    },
    "papermill": {
     "duration": 0.045731,
     "end_time": "2022-03-07T14:43:45.349002",
     "exception": false,
     "start_time": "2022-03-07T14:43:45.303271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1min 34s\n",
    "if False:\n",
    "    model = LogisticRegression(C=1, dual=True, solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # if hasattr(model, 'oob_score_'): print(model.oob_score_)\n",
    "\n",
    "    # model = MultinomialNB()\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # print(model.score(X_train, y_train))\n",
    "    # print(model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60e503d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T14:43:45.469150Z",
     "iopub.status.busy": "2022-03-07T14:43:45.424323Z",
     "iopub.status.idle": "2022-03-07T15:35:45.390923Z",
     "shell.execute_reply": "2022-03-07T15:35:45.390320Z"
    },
    "papermill": {
     "duration": 3120.005265,
     "end_time": "2022-03-07T15:35:45.391096",
     "exception": false,
     "start_time": "2022-03-07T14:43:45.385831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:260: UserWarning: 'evals_result' argument is deprecated and will be removed in a future release of LightGBM. Pass 'record_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'evals_result' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 262.466355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 257655\n",
      "[LightGBM] [Info] Number of data points in the train set: 168222, number of used features: 100003\n",
      "[LightGBM] [Info] Start training from score -1.215524\n",
      "[LightGBM] [Info] Start training from score -2.528464\n",
      "[LightGBM] [Info] Start training from score -3.371400\n",
      "[LightGBM] [Info] Start training from score -1.309464\n",
      "[LightGBM] [Info] Start training from score -1.912709\n",
      "[LightGBM] [Info] Start training from score -2.900121\n",
      "[LightGBM] [Info] Start training from score -2.396060\n",
      "[LightGBM] [Info] Start training from score -3.665740\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tTrain's multi_logloss: 0.864077\tValid's multi_logloss: 0.940988\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tTrain's multi_logloss: 0.864077\tValid's multi_logloss: 0.940988\n",
      "CPU times: user 1h 37min 30s, sys: 6.53 s, total: 1h 37min 37s\n",
      "Wall time: 51min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "params_lgb = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_class': len(set(y_train)),\n",
    "    'n_estimators': 100,\n",
    "    'random_state': 0,\n",
    "    'learning_rate': 0.05,\n",
    "    'early_stopping_rounds': 50,\n",
    "    'subsample': 0.6,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.4,\n",
    "    'reg_alpha': 10.0,\n",
    "    'reg_lambda': 1e-1,\n",
    "    'min_child_weight': 256,\n",
    "    'min_child_samples': 4,\n",
    "    'device': 'cpu',\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "result = {}\n",
    "model_lgb = lgb.train(params_lgb,\n",
    "                      lgb_train,\n",
    "                      valid_sets=[lgb_train, lgb_eval],\n",
    "                      valid_names=['Train', 'Valid'],\n",
    "                      verbose_eval=100,\n",
    "                      num_boost_round=500,\n",
    "                      evals_result=result)\n",
    "\n",
    "X_val_pred = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1589200",
   "metadata": {
    "papermill": {
     "duration": 0.213949,
     "end_time": "2022-03-07T15:35:45.824582",
     "exception": false,
     "start_time": "2022-03-07T15:35:45.610633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction\n",
    "\n",
    "## Designing The Algorithm\n",
    "\n",
    "The algorithm should take a full text and then output a dataframe of classes and prediction strings (tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a74a95d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:46.264003Z",
     "iopub.status.busy": "2022-03-07T15:35:46.263425Z",
     "iopub.status.idle": "2022-03-07T15:35:46.311484Z",
     "shell.execute_reply": "2022-03-07T15:35:46.311914Z"
    },
    "papermill": {
     "duration": 0.271092,
     "end_time": "2022-03-07T15:35:46.312064",
     "exception": false,
     "start_time": "2022-03-07T15:35:46.040972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "      <th>start_token</th>\n",
       "      <th>len_essay</th>\n",
       "      <th>next_discourse_type</th>\n",
       "      <th>next_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>379</td>\n",
       "      <td>Position</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "      <td>45</td>\n",
       "      <td>379</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>60</td>\n",
       "      <td>379</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \\\n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...   \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59   \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "\n",
       "   start_token  len_essay next_discourse_type       next_id  \n",
       "0            1        379            Position  423A1CA112E2  \n",
       "1           45        379            Evidence  423A1CA112E2  \n",
       "2           60        379            Evidence  423A1CA112E2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 次の discourse_type, 次の id を意味する列を追加\n",
    "dev_df['next_discourse_type'] = dev_df['discourse_type'].shift(-1)\n",
    "dev_df['next_id'] = dev_df['id'].shift(-1)\n",
    "\n",
    "dev_df.loc[\n",
    "    dev_df['next_id'] != dev_df['id'],\n",
    "    'next_discourse_type'\n",
    "] = 'NaN'\n",
    "\n",
    "dev_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ab64fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:46.755247Z",
     "iopub.status.busy": "2022-03-07T15:35:46.754462Z",
     "iopub.status.idle": "2022-03-07T15:35:46.805731Z",
     "shell.execute_reply": "2022-03-07T15:35:46.806175Z"
    },
    "papermill": {
     "duration": 0.276465,
     "end_time": "2022-03-07T15:35:46.806325",
     "exception": false,
     "start_time": "2022-03-07T15:35:46.529860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>next_discourse_type</th>\n",
       "      <th>Claim</th>\n",
       "      <th>Concluding Statement</th>\n",
       "      <th>Counterclaim</th>\n",
       "      <th>Evidence</th>\n",
       "      <th>Gap</th>\n",
       "      <th>Lead</th>\n",
       "      <th>Position</th>\n",
       "      <th>Rebuttal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourse_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Claim</th>\n",
       "      <td>0.198729</td>\n",
       "      <td>0.018021</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.559825</td>\n",
       "      <td>0.189268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Concluding Statement</th>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017063</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Counterclaim</th>\n",
       "      <td>0.033928</td>\n",
       "      <td>0.025446</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.224338</td>\n",
       "      <td>0.070106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.618141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evidence</th>\n",
       "      <td>0.383538</td>\n",
       "      <td>0.187629</td>\n",
       "      <td>0.072995</td>\n",
       "      <td>0.092394</td>\n",
       "      <td>0.201304</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.010393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gap</th>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.098539</td>\n",
       "      <td>0.022783</td>\n",
       "      <td>0.277422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050034</td>\n",
       "      <td>0.009822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lead</th>\n",
       "      <td>0.091734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016099</td>\n",
       "      <td>0.043220</td>\n",
       "      <td>0.108806</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.739600</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Position</th>\n",
       "      <td>0.460153</td>\n",
       "      <td>0.032243</td>\n",
       "      <td>0.021278</td>\n",
       "      <td>0.171986</td>\n",
       "      <td>0.279943</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rebuttal</th>\n",
       "      <td>0.159154</td>\n",
       "      <td>0.210967</td>\n",
       "      <td>0.052045</td>\n",
       "      <td>0.412872</td>\n",
       "      <td>0.113151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.002556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "next_discourse_type      Claim  Concluding Statement  Counterclaim  Evidence  \\\n",
       "discourse_type                                                                 \n",
       "Claim                 0.198729              0.018021      0.020226  0.559825   \n",
       "Concluding Statement  0.005439              0.000224      0.007675  0.002310   \n",
       "Counterclaim          0.033928              0.025446      0.011425  0.224338   \n",
       "Evidence              0.383538              0.187629      0.072995  0.092394   \n",
       "Gap                   0.541400              0.098539      0.022783  0.277422   \n",
       "Lead                  0.091734              0.000000      0.016099  0.043220   \n",
       "Position              0.460153              0.032243      0.021278  0.171986   \n",
       "Rebuttal              0.159154              0.210967      0.052045  0.412872   \n",
       "\n",
       "next_discourse_type        Gap      Lead  Position  Rebuttal  \n",
       "discourse_type                                                \n",
       "Claim                 0.189268  0.000000  0.008259  0.000040  \n",
       "Concluding Statement  0.005290  0.000000  0.017063  0.000000  \n",
       "Counterclaim          0.070106  0.000000  0.011425  0.618141  \n",
       "Evidence              0.201304  0.000066  0.016537  0.010393  \n",
       "Gap                   0.000000  0.000000  0.050034  0.009822  \n",
       "Lead                  0.108806  0.000108  0.739600  0.000108  \n",
       "Position              0.279943  0.000196  0.000065  0.000196  \n",
       "Rebuttal              0.113151  0.000000  0.013708  0.002556  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 推移確率行列\n",
    "discourse_next = pd.pivot_table(data=dev_df, index='discourse_type', columns='next_discourse_type', aggfunc='size')\n",
    "discourse_next = discourse_next.apply(lambda x: x / discourse_next.sum(axis=1))\n",
    "discourse_next = discourse_next[encoder.classes_].fillna(0)\n",
    "discourse_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70ab58ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:47.256366Z",
     "iopub.status.busy": "2022-03-07T15:35:47.255765Z",
     "iopub.status.idle": "2022-03-07T15:35:47.258447Z",
     "shell.execute_reply": "2022-03-07T15:35:47.258835Z"
    },
    "papermill": {
     "duration": 0.22995,
     "end_time": "2022-03-07T15:35:47.258993",
     "exception": false,
     "start_time": "2022-03-07T15:35:47.029043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Claim': array([1.98729128e-01, 1.80207268e-02, 2.02257101e-02, 5.59825205e-01,\n",
       "        1.89267745e-01, 0.00000000e+00, 8.25866458e-03, 4.00906048e-05]),\n",
       " 'Concluding Statement': array([0.00543924, 0.00022353, 0.00767454, 0.00230981, 0.00529022,\n",
       "        0.        , 0.01706281, 0.        ]),\n",
       " 'Counterclaim': array([0.03392764, 0.02544573, 0.01142461, 0.22433789, 0.07010559,\n",
       "        0.        , 0.01142461, 0.6181409 ]),\n",
       " 'Evidence': array([3.83538116e-01, 1.87629365e-01, 7.29951116e-02, 9.23944158e-02,\n",
       "        2.01303563e-01, 6.60589246e-05, 1.65367508e-02, 1.03932708e-02]),\n",
       " 'Gap': array([0.54139999, 0.09853882, 0.02278308, 0.27742221, 0.        ,\n",
       "        0.        , 0.05003421, 0.00982168]),\n",
       " 'Lead': array([9.17341977e-02, 0.00000000e+00, 1.60994057e-02, 4.32198811e-02,\n",
       "        1.08806051e-01, 1.08049703e-04, 7.39600216e-01, 1.08049703e-04]),\n",
       " 'Position': array([4.60152732e-01, 3.22433262e-02, 2.12779845e-02, 1.71986163e-01,\n",
       "        2.79942562e-01, 1.95809673e-04, 6.52698910e-05, 1.95809673e-04]),\n",
       " 'Rebuttal': array([0.15915428, 0.21096654, 0.05204461, 0.41287175, 0.11315056,\n",
       "        0.        , 0.01370818, 0.00255576])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 推移確率行列を辞書化\n",
    "expectations = {}\n",
    "for discourse_type, row in discourse_next.iterrows():\n",
    "    expectations[discourse_type] = row.values\n",
    "\n",
    "expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a2f77d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:47.782737Z",
     "iopub.status.busy": "2022-03-07T15:35:47.781835Z",
     "iopub.status.idle": "2022-03-07T15:35:48.248351Z",
     "shell.execute_reply": "2022-03-07T15:35:48.249043Z"
    },
    "papermill": {
     "duration": 0.767217,
     "end_time": "2022-03-07T15:35:48.249257",
     "exception": false,
     "start_time": "2022-03-07T15:35:47.482040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 隠れマルコフモデルっぽいことをしている\n",
    "# 訓練データの推移確率行列を使って、前の予測 discourse_type から、次に出現しやすい discourse_type の情報を使っている\n",
    "\n",
    "import nltk\n",
    "\n",
    "def predict_text(model, data, pipeline):\n",
    "    # return model.predict_proba(pipeline.transform(data))\n",
    "    return model_lgb.predict(pipeline.transform(data), num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "def predict_essay(model, essay_id, path, pipeline, max_iter, print_results=False):\n",
    "    essay_txt = read_essay_txt(essay_id, path)\n",
    "    essay_sentences = nltk.sent_tokenize(essay_txt)\n",
    "    essay_preds = {}\n",
    "    essay_preds['id'] = essay_id\n",
    "    essay_preds['discourse_type'] = []\n",
    "    essay_preds['predictionstring'] = []\n",
    "    \n",
    "#     print(len(essay_sentences))\n",
    "#     print(len(essay_txt.split()))\n",
    "    \n",
    "    start_token = 0\n",
    "    end_token = 0\n",
    "    start_sent = 0\n",
    "    end_sent = 1\n",
    "    iter_bad = 0\n",
    "    max_pred = 0\n",
    "    \n",
    "    start_end_sents, max_preds, argmax_preds = [], [], [] \n",
    "    stop = False\n",
    "    \n",
    "    prev_type = None\n",
    "    \n",
    "    while not stop: \n",
    "        data = {}\n",
    "        data['start_token'] = [start_token + 1]\n",
    "        data['len_essay'] = train_essays_length[essay_id] if path == 'train' else test_essays_length[essay_id]\n",
    "        data['discourse_text'] = [' '.join(essay_sentences[start_sent:end_sent])]\n",
    "        data = pd.DataFrame(data)[['start_token', 'len_essay', 'discourse_text']]\n",
    "        preds = predict_text(model, data, pipeline)\n",
    "        \n",
    "#         print('Before update:', np.argmax(preds))\n",
    "        if prev_type:\n",
    "            preds = update_preds(preds, prev_type)\n",
    "            \n",
    "#         print('After update:', np.argmax(preds), prev_type)\n",
    "\n",
    "        if preds.max() >= max_pred:\n",
    "            max_pred = preds.max()\n",
    "            start_end_sents.append([start_sent, end_sent])\n",
    "            max_preds.append(max_pred)\n",
    "            argmax_preds.append(preds.argmax())\n",
    "\n",
    "        else:\n",
    "            iter_bad += 1\n",
    "        \n",
    "#         print(start_sent, end_sent, encoder.inverse_transform([preds.argmax()])[0], preds.max(), max_pred, iter_bad)\n",
    "        end_sent += 1\n",
    "            \n",
    "#         print(start_end_sents, max_preds)\n",
    "                         \n",
    "        if iter_bad >= max_iter or end_sent > len(essay_sentences):\n",
    "            best_pred = np.argmax(max_preds)\n",
    "            best_start_end = start_end_sents[best_pred]\n",
    "            merged_sentence = ' '.join(essay_sentences[best_start_end[0]: best_start_end[-1]])\n",
    "            end_token = len(merged_sentence.split()) + end_token\n",
    "            prediction_string = ' '.join([str(token) for token in range(start_token, end_token)])\n",
    "            \n",
    "            essay_preds['discourse_type'].append(encoder.inverse_transform([argmax_preds[best_pred]])[0])\n",
    "            essay_preds['predictionstring'].append(prediction_string)\n",
    "            \n",
    "            if print_results: print('MATCH ------- \\n', merged_sentence, '\\n\\n', encoder.inverse_transform([argmax_preds[best_pred]])[0], '\\n\\n')\n",
    "            \n",
    "            start_token = end_token\n",
    "            start_sent = best_start_end[-1]\n",
    "            \n",
    "#             end_token = start_token + 1\n",
    "            end_sent = start_sent + 1\n",
    "            \n",
    "            iter_bad = 0\n",
    "            max_pred = 0\n",
    "            \n",
    "            prev_type = encoder.inverse_transform([argmax_preds[best_pred]])[0]\n",
    "            \n",
    "            start_end_sents, max_preds, argmax_preds = [], [], []\n",
    "            \n",
    "            \n",
    "        if start_sent == len(essay_sentences):\n",
    "            stop = True\n",
    "    \n",
    "    return essay_preds\n",
    "\n",
    "def update_preds(preds, prev_type):\n",
    "    prev_type_expec = expectations[prev_type]\n",
    "    return preds * prev_type_expec\n",
    "\n",
    "def predict_df(model, df, path, pipeline, max_iter=5):\n",
    "    essay_ids = df['id'].unique()\n",
    "    preds_df = None\n",
    "    for essay_id in essay_ids:\n",
    "        essay_preds = predict_essay(model, essay_id, path, pipeline, max_iter)\n",
    "        preds_df = pd.concat([preds_df, pd.DataFrame(essay_preds)], axis=0)\n",
    "        \n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59393437",
   "metadata": {
    "papermill": {
     "duration": 0.251978,
     "end_time": "2022-03-07T15:35:48.863876",
     "exception": false,
     "start_time": "2022-03-07T15:35:48.611898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's test the algorithm now with one essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd905512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:49.312237Z",
     "iopub.status.busy": "2022-03-07T15:35:49.311360Z",
     "iopub.status.idle": "2022-03-07T15:35:49.314335Z",
     "shell.execute_reply": "2022-03-07T15:35:49.314722Z"
    },
    "papermill": {
     "duration": 0.228616,
     "end_time": "2022-03-07T15:35:49.314875",
     "exception": false,
     "start_time": "2022-03-07T15:35:49.086259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['570D8769BE33', 'B18FB042DEDD', 'B5DE2FAE1DB5', '0FB84B0726F5',\n",
       "       '656F48B15786', '8ABA260B3B98', '1E8C298D92CB', '63001194BE9C',\n",
       "       '535D71E8C000', '0F92CE19A137', '2E1266682F4A', '91A7412303BD',\n",
       "       'CD10A87243D1', '6BFE834C4FD5', '16EAB34FDC87', 'CC50C8238771',\n",
       "       'BE80D73EC131', '714F75BBE481', 'E70DA4483D3F', '130B05555BA1',\n",
       "       '983273B60F84', '003FDC7E6F20', 'A5DA0D5410DE', 'D69416DBA6F4',\n",
       "       '0A6E7B9813A4', '1B241587A2A6', '476E07AEA488', 'F324DBEBCAFA',\n",
       "       'C60B2BD410BD', '64551673D7DB', 'A8A25F94F5B6', '10F1ACB5559E',\n",
       "       '64EE7AB92E51', 'D2A7CA1EEFC5', '25FA01853A35', 'C237996BF240',\n",
       "       'E66C66D98801', '9AFE16DB63D8', '4441EEF0E6FD', '1627A070AEBB',\n",
       "       '04F277F6562D', '32D988FC1AA1', 'AA3A35044E77', '0CE521F8D172',\n",
       "       '3B053B2F03A3', '8DBEE30EF6A7', '318E29BC6F06', '649807ACDD9F',\n",
       "       'E3BA4F414829', '85EB8DC94BD0', '0B4FAC7A4A8B', 'B88301B4E6D7',\n",
       "       'B15EA1EE9302', 'B23941DFCB72', '1AC3332D2C41', '520B38139E21',\n",
       "       'F6BD9804DD63', 'A62DC876A1A9', '370577117FFA', '23F613EA5253',\n",
       "       'AC2463899B0F', 'B67DFD07510C', '6F136BBBBF5C', '0347FB2B37A9',\n",
       "       'EFB450A2F170', 'F60C9A8B9651', 'D0187B1F88F7', '008B27E80228',\n",
       "       '11D9263201FD', '98FD2BF096A9', 'F095852ED985', '6F563DEA1D26',\n",
       "       'C9DCBE24604E', '0E6ADE74D132', 'E079C8DE0D1E', '91B00B44402B',\n",
       "       'A385801FF637', '2421208F6A6E', '1160237A1858', 'B3813CAD3B8D',\n",
       "       '5CB615A9027C', '84B1E0E85878', '417F362A7B19', '180159E53733',\n",
       "       '27323553A085', '5742BBAD7FAA', 'D450BF08BE17', 'D1B97265EAEF',\n",
       "       'FE01BAE235A1', 'C21F5FA26FC7', 'BDE08546F7EE', '857D1D3BC2C6',\n",
       "       '505A24640C8D', '57D8DC790742', '92C94CAD934B', '9BC192B91D83',\n",
       "       '6F29B2264805', '5C9E6CCEAF57', 'A8DFF4D30133', '0E1251EF8544'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba51891b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:49.760717Z",
     "iopub.status.busy": "2022-03-07T15:35:49.759962Z",
     "iopub.status.idle": "2022-03-07T15:35:50.519152Z",
     "shell.execute_reply": "2022-03-07T15:35:50.520080Z"
    },
    "papermill": {
     "duration": 0.986108,
     "end_time": "2022-03-07T15:35:50.520262",
     "exception": false,
     "start_time": "2022-03-07T15:35:49.534154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH ------- \n",
      " Should drivers be able to use a phone in any capacity while operating a vehicle or should drivers not be able to use their phones while operating a vehicle? Using a phone while driving could be helpful in various ways and also could be used in harmful ways. Cell phones can take the attention of a driver causing a risk of danger. A cell phone helps drivers with immediate roadside assistance, in case of a serious car accident. Cell phone's allow drivers to get directions turn by turn with the helping of GPS technology. Phone cameras are very important in a case of a hit and run accident, it allows a person to take a picture of the license plate. \n",
      "\n",
      " Evidence \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " Some insurance companies may require a picture of an accident. Traffic reports update on a phone in minutes, accidents, highway closings and also an estimated travel time to their destination. \n",
      "\n",
      " Claim \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " An activity that grabs the attention from a driver is Distracted Driving. Being distracted on the road causes fatal crashes. Many jurisdictions worldwide have made calling an illegal act. However lots of vehicles have hands-free devices, some may say that hands-free activities are not safe. Hands-free still drags away the attention from a driver on the road \"Distracted Driving\". Currently, cell phones are the leading cause of distracted driving. When operating a cell phone while driving increases hospitalization rate four-times higher. Texting increases the rate to 23%\n",
      "\n",
      "more likely to have a car accident. \n",
      "\n",
      " Evidence \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " Personally, I think cellphones should not be used in any capacity while operating a vehicle. There are many accidents caused by the use of a cell phone. \n",
      "\n",
      " Claim \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " A person who operates a cell phone while driving is not only putting his or herself in danger but also others on the road and pedestrians. Using a cell phone while operating a vehicle is selfish because the lack of control a person has on the road. \n",
      "\n",
      " Evidence \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " My essay provided arguments from both stand points whether drivers should or should not be able to use cell phones while operating a vehicle. On the facts provided throughout my essay, despite the many ways of communication cell phones while driving raise many safety issues. \n",
      "\n",
      " Concluding Statement \n",
      "\n",
      "\n",
      "MATCH ------- \n",
      " In conclude the bad outweighed the good. \n",
      "\n",
      " Position \n",
      "\n",
      "\n",
      "CPU times: user 1.08 s, sys: 7.01 ms, total: 1.09 s\n",
      "Wall time: 755 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "_ = predict_essay(model_lgb, '570D8769BE33', 'train', pipeline, 2, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8867f09",
   "metadata": {
    "papermill": {
     "duration": 0.222699,
     "end_time": "2022-03-07T15:35:50.967357",
     "exception": false,
     "start_time": "2022-03-07T15:35:50.744658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "According to the competition evaluation page\n",
    "\n",
    "1. For each sample, all ground truths and predictions for a given class are compared.\n",
    "2. If the overlap between the ground truth and prediction is >= 0.5, and the overlap between the prediction and the ground truth >= 0.5, the prediction is a match and considered a true positive. If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "3. Any unmatched ground truths are false negatives and any unmatched predictions are false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "082ebc50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:35:51.417332Z",
     "iopub.status.busy": "2022-03-07T15:35:51.416480Z",
     "iopub.status.idle": "2022-03-07T15:36:22.159702Z",
     "shell.execute_reply": "2022-03-07T15:36:22.158903Z"
    },
    "papermill": {
     "duration": 30.968586,
     "end_time": "2022-03-07T15:36:22.159933",
     "exception": false,
     "start_time": "2022-03-07T15:35:51.191347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.7 s, sys: 209 ms, total: 57.9 s\n",
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570D8769BE33</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570D8769BE33</td>\n",
       "      <td>Claim</td>\n",
       "      <td>120 121 122 123 124 125 126 127 128 129 130 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570D8769BE33</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>150 151 152 153 154 155 156 157 158 159 160 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570D8769BE33</td>\n",
       "      <td>Claim</td>\n",
       "      <td>244 245 246 247 248 249 250 251 252 253 254 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570D8769BE33</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>271 272 273 274 275 276 277 278 279 280 281 28...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id discourse_type  \\\n",
       "0  570D8769BE33       Evidence   \n",
       "1  570D8769BE33          Claim   \n",
       "2  570D8769BE33       Evidence   \n",
       "3  570D8769BE33          Claim   \n",
       "4  570D8769BE33       Evidence   \n",
       "\n",
       "                                    predictionstring  \n",
       "0  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "1  120 121 122 123 124 125 126 127 128 129 130 13...  \n",
       "2  150 151 152 153 154 155 156 157 158 159 160 16...  \n",
       "3  244 245 246 247 248 249 250 251 252 253 254 25...  \n",
       "4  271 272 273 274 275 276 277 278 279 280 281 28...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 30.5s\n",
    "val_preds_df = predict_df(model_lgb, val_df, 'train', pipeline, 2)\n",
    "val_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09498359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:22.626484Z",
     "iopub.status.busy": "2022-03-07T15:36:22.625665Z",
     "iopub.status.idle": "2022-03-07T15:36:22.627756Z",
     "shell.execute_reply": "2022-03-07T15:36:22.628199Z"
    },
    "papermill": {
     "duration": 0.23956,
     "end_time": "2022-03-07T15:36:22.628340",
     "exception": false,
     "start_time": "2022-03-07T15:36:22.388780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_df(df, pred_df):\n",
    "    essay_ids = df['id'].unique()\n",
    "    f1_scores = []\n",
    "    for essay_id in essay_ids:\n",
    "        f1_score = evaluate_essay(df, pred_df, essay_id)\n",
    "        f1_scores.append(f1_score)\n",
    "    return np.mean(f1_scores)\n",
    "        \n",
    "def evaluate_essay(df, pred_df, essay_id, print_results=False):\n",
    "    essay_df = filter_essay(df, essay_id)\n",
    "    pred_essay_df = filter_essay(pred_df, essay_id)\n",
    "    pred_essay_df = pred_essay_df.loc[pred_essay_df['discourse_type'] != 'Gap', :]\n",
    "    f1_scores = []\n",
    "    for class_ in df['discourse_type'].unique():\n",
    "        f1_score = evaluate_class(essay_df, pred_essay_df, class_, print_results)\n",
    "        f1_scores.append(f1_score)\n",
    "        \n",
    "    return np.mean(f1_scores)\n",
    "        \n",
    "def evaluate_class(df, pred_df, class_, print_results):\n",
    "    class_df = filter_class(df, class_)\n",
    "    pred_class_df = filter_class(pred_df, class_)\n",
    "    truths = class_df['predictionstring'].str.split(' ').tolist()\n",
    "    predictions = pred_class_df['predictionstring'].str.split(' ').tolist()\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    matched_truths_idx = []\n",
    "    for prediction in predictions:\n",
    "        for i, truth in enumerate(truths):\n",
    "            if test_overlap(prediction, truth):\n",
    "                true_positives += 1 \n",
    "                matched_truths_idx.append(i)\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        truths = remove_from_list(truths, matched_truths_idx)\n",
    "        matched_truths_idx = []\n",
    "        \n",
    "    false_negatives = len(truths)\n",
    "    \n",
    "    f1_score = calculate_f1(true_positives, false_positives, false_negatives)\n",
    "    \n",
    "    if print_results: print(class_, f1_score)\n",
    "        \n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def filter_class(df, class_):\n",
    "    return df.query('discourse_type == @class_')\n",
    "                \n",
    "        \n",
    "def test_overlap(prediction, truth):\n",
    "    prediction_set = set(prediction)\n",
    "    truth_set = set(truth)\n",
    "#     print(overlap_fraction(prediction_set, truth_set), overlap_fraction(truth_set, prediction_set))\n",
    "    if overlap_fraction(prediction_set, truth_set) >= 0.5 and overlap_fraction(truth_set, prediction_set) >= 0.5:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def overlap_fraction(set1, set2):\n",
    "    return len(set1.intersection(set2)) / len(set1)\n",
    "    \n",
    "    \n",
    "def remove_from_list(list_, idx):\n",
    "    return [x for i, x in enumerate(list_) if i not in idx]\n",
    "\n",
    "def calculate_f1(true_positives, false_positives, false_negatives):\n",
    "    precision = calculate_precision(true_positives, false_positives)\n",
    "    recall = calculate_recall(true_positives, false_negatives)\n",
    "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    \n",
    "def calculate_precision(true_positives, false_positives):\n",
    "    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "def calculate_recall(true_positives, false_negatives):\n",
    "    return true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb3d316d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:23.117504Z",
     "iopub.status.busy": "2022-03-07T15:36:23.086371Z",
     "iopub.status.idle": "2022-03-07T15:36:23.124395Z",
     "shell.execute_reply": "2022-03-07T15:36:23.125079Z"
    },
    "papermill": {
     "duration": 0.276885,
     "end_time": "2022-03-07T15:36:23.125251",
     "exception": false,
     "start_time": "2022-03-07T15:36:22.848366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead 0\n",
      "Claim 0\n",
      "Evidence 0.125\n",
      "Counterclaim 0\n",
      "Position 0\n",
      "Concluding Statement 1.0\n",
      "Rebuttal 0\n",
      "Gap 0\n",
      "CPU times: user 42.1 ms, sys: 1.03 ms, total: 43.2 ms\n",
      "Wall time: 40.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.140625"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Test one essay\n",
    "evaluate_essay(val_df, val_preds_df, '570D8769BE33', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90ff9fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:23.967312Z",
     "iopub.status.busy": "2022-03-07T15:36:23.757906Z",
     "iopub.status.idle": "2022-03-07T15:36:26.767259Z",
     "shell.execute_reply": "2022-03-07T15:36:26.767839Z"
    },
    "papermill": {
     "duration": 3.418161,
     "end_time": "2022-03-07T15:36:26.768041",
     "exception": false,
     "start_time": "2022-03-07T15:36:23.349880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.2 s, sys: 28.2 ms, total: 3.23 s\n",
      "Wall time: 3.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08505312253106372"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_df(val_df, val_preds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8f43a",
   "metadata": {
    "papermill": {
     "duration": 0.234462,
     "end_time": "2022-03-07T15:36:27.229647",
     "exception": false,
     "start_time": "2022-03-07T15:36:26.995185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "530c7993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:27.681792Z",
     "iopub.status.busy": "2022-03-07T15:36:27.681258Z",
     "iopub.status.idle": "2022-03-07T15:36:27.739757Z",
     "shell.execute_reply": "2022-03-07T15:36:27.739261Z"
    },
    "papermill": {
     "duration": 0.286228,
     "end_time": "2022-03-07T15:36:27.739918",
     "exception": false,
     "start_time": "2022-03-07T15:36:27.453690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ids = !ls '/kaggle/input/feedback-prize-2021/test'\n",
    "test_ids = [id_.rstrip('.txt') for id_ in test_ids]\n",
    "test_df = pd.DataFrame({'id': test_ids})\n",
    "test_df['len_essay'] = test_df['id'].map(test_essays_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e77d03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:28.469827Z",
     "iopub.status.busy": "2022-03-07T15:36:28.469009Z",
     "iopub.status.idle": "2022-03-07T15:36:31.086428Z",
     "shell.execute_reply": "2022-03-07T15:36:31.087580Z"
    },
    "papermill": {
     "duration": 2.957693,
     "end_time": "2022-03-07T15:36:31.087817",
     "exception": false,
     "start_time": "2022-03-07T15:36:28.130124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 s, sys: 19 ms, total: 5.18 s\n",
      "Wall time: 2.61 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>249 250 251 252 253 254 255 256 257 258 259 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>262 263 264 265 266 267 268 269 270 271 272 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>432 433 434 435 436 437 438 439 440 441 442 44...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id discourse_type  \\\n",
       "0  0FB0700DAF44          Claim   \n",
       "1  0FB0700DAF44       Evidence   \n",
       "2  0FB0700DAF44          Claim   \n",
       "3  0FB0700DAF44       Evidence   \n",
       "4  0FB0700DAF44          Claim   \n",
       "\n",
       "                                    predictionstring  \n",
       "0              0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  \n",
       "1  16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 3...  \n",
       "2  249 250 251 252 253 254 255 256 257 258 259 26...  \n",
       "3  262 263 264 265 266 267 268 269 270 271 272 27...  \n",
       "4  432 433 434 435 436 437 438 439 440 441 442 44...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "submission = predict_df(model_lgb, test_df, 'test', pipeline, 2)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2c66f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:31.545184Z",
     "iopub.status.busy": "2022-03-07T15:36:31.544372Z",
     "iopub.status.idle": "2022-03-07T15:36:31.546242Z",
     "shell.execute_reply": "2022-03-07T15:36:31.546712Z"
    },
    "papermill": {
     "duration": 0.228466,
     "end_time": "2022-03-07T15:36:31.546837",
     "exception": false,
     "start_time": "2022-03-07T15:36:31.318371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.columns = ['id', 'class', 'predictionstring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c006ce56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:31.996865Z",
     "iopub.status.busy": "2022-03-07T15:36:31.996016Z",
     "iopub.status.idle": "2022-03-07T15:36:31.997773Z",
     "shell.execute_reply": "2022-03-07T15:36:31.998298Z"
    },
    "papermill": {
     "duration": 0.229126,
     "end_time": "2022-03-07T15:36:31.998434",
     "exception": false,
     "start_time": "2022-03-07T15:36:31.769308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop gaps\n",
    "submission = submission.loc[submission['class'] != 'Gap', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73c141b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T15:36:32.453210Z",
     "iopub.status.busy": "2022-03-07T15:36:32.451637Z",
     "iopub.status.idle": "2022-03-07T15:36:32.458671Z",
     "shell.execute_reply": "2022-03-07T15:36:32.459083Z"
    },
    "papermill": {
     "duration": 0.234616,
     "end_time": "2022-03-07T15:36:32.459233",
     "exception": false,
     "start_time": "2022-03-07T15:36:32.224617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3624.655308,
   "end_time": "2022-03-07T15:36:34.809839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-07T14:36:10.154531",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
